{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/HandRefiner-colab/blob/main/HandRefiner_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/HandRefiner\n",
        "%cd /content/HandRefiner\n",
        "\n",
        "!pip install -q pytorch-lightning omegaconf einops yacs trimesh mediapipe rtree boto3 gradio==3.50.2\n",
        "!pip install -q git+https://gitlab.eecs.umich.edu/ngv-python-modules/opendr\n",
        "!pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "%cd /content/HandRefiner\n",
        "!git clone -b dev --recursive https://github.com/camenduru/MeshGraphormer\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/basicModel_neutral_lbs_10_207_0_v1.0.0.pkl -d /content/HandRefiner/MeshGraphormer/src/modeling/data -o basicModel_neutral_lbs_10_207_0_v1.0.0.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/MANO_RIGHT.pkl -d /content/HandRefiner/MeshGraphormer/src/modeling/data -o MANO_RIGHT.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/graphormer_release/graphormer_3dpw_state_dict.bin -d /content/HandRefiner/MeshGraphormer/models/graphormer_release -o graphormer_3dpw_state_dict.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/graphormer_release/graphormer_h36m_state_dict.bin -d /content/HandRefiner/MeshGraphormer/models/graphormer_release -o graphormer_h36m_state_dict.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/graphormer_release/graphormer_hand_state_dict.bin -d /content/HandRefiner/MeshGraphormer/models/graphormer_release -o graphormer_hand_state_dict.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/raw/main/hrnet/cls_hrnet_w40_sgd_lr5e-2_wd1e-4_bs32_x100.yaml -d /content/HandRefiner/MeshGraphormer/models/hrnet -o cls_hrnet_w40_sgd_lr5e-2_wd1e-4_bs32_x100.yaml\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/raw/main/hrnet/cls_hrnet_w64_sgd_lr5e-2_wd1e-4_bs32_x100.yaml -d /content/HandRefiner/MeshGraphormer/models/hrnet -o cls_hrnet_w64_sgd_lr5e-2_wd1e-4_bs32_x100.yaml\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/hrnet/hrnetv2_w40_imagenet_pretrained.pth -d /content/HandRefiner/MeshGraphormer/models/hrnet -o hrnetv2_w40_imagenet_pretrained.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/hrnet/hrnetv2_w64_imagenet_pretrained.pth -d /content/HandRefiner/MeshGraphormer/models/hrnet -o hrnetv2_w64_imagenet_pretrained.pth\n",
        "%cd /content/HandRefiner/MeshGraphormer\n",
        "!pip install -e .\n",
        "!pip install -e ./manopth/.\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/hand_landmarker.task -d /content/HandRefiner/preprocessor -o hand_landmarker.task\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/inpaint_depth_control.ckpt -d /content/HandRefiner/models -o inpaint_depth_control.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/HandRefiner/resolve/main/ckpt200-multisc-pred.zip -d /content/HandRefiner/MeshGraphormer/predictions -o ckpt200-multisc-pred.zip\n",
        "!mkdir /content/HandRefiner/output\n",
        "%cd /content/HandRefiner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/HandRefiner\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import sys\n",
        "from config import handrefiner_root\n",
        "import os\n",
        "\n",
        "def load():\n",
        "    paths = [handrefiner_root, os.path.join(handrefiner_root, 'MeshGraphormer'), os.path.join(handrefiner_root, 'preprocessor')]\n",
        "    for p in paths:\n",
        "        sys.path.insert(0, p)\n",
        "\n",
        "load()\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from pytorch_lightning import seed_everything\n",
        "from cldm.model import create_model, load_state_dict\n",
        "from cldm.ddim_hacked import DDIMSampler\n",
        "import config\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from preprocessor.meshgraphormer import MeshGraphormerMediapipe\n",
        "import ast\n",
        "\n",
        "transform = transforms.Compose([           \n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(\n",
        "                        mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "model = create_model(\"control_depth_inpaint.yaml\").cpu()\n",
        "model.load_state_dict(load_state_dict('/content/HandRefiner/models/inpaint_depth_control.ckpt', location='cuda'), strict=False)\n",
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(image_path, prompt, seed):\n",
        "    meshgraphormer = MeshGraphormerMediapipe()\n",
        "    image = np.array(Image.open(image_path))\n",
        "    raw_image = image\n",
        "    H, W, C = raw_image.shape\n",
        "    gen_count = 0\n",
        "\n",
        "    file_name_raw = Path(image_path).stem\n",
        "    depthmap, mask, info = meshgraphormer.get_depth(os.path.dirname(image_path), os.path.basename(image_path), 30)\n",
        "    cv2.imwrite(\"img_depth.jpg\", depthmap)\n",
        "    cv2.imwrite(\"img_mask.jpg\", mask)\n",
        "    control = depthmap\n",
        "\n",
        "    ddim_sampler = DDIMSampler(model)\n",
        "    out_dir=\"/content/HandRefiner/output\"\n",
        "    num_samples = 1\n",
        "    ddim_steps = 50\n",
        "    guess_mode = False\n",
        "    adaptive_control = False\n",
        "    eval=False\n",
        "    strength = 1.0\n",
        "    scale = 9.0\n",
        "\n",
        "    a_prompt = \"realistic, best quality, extremely detailed\"\n",
        "    n_prompt = \"fake 3D rendered image, longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, blue\"\n",
        "\n",
        "    source = raw_image\n",
        "\n",
        "    source = (source.astype(np.float32) / 127.5) - 1.0\n",
        "    source = source.transpose([2, 0, 1])  # source is c h w\n",
        "\n",
        "    mask = mask.astype(np.float32) / 255.0\n",
        "    mask = mask[None]\n",
        "    mask[mask < 0.5] = 0\n",
        "    mask[mask >= 0.5] = 1\n",
        "\n",
        "    hint = control.astype(np.float32) / 255.0\n",
        "\n",
        "    masked_image = source * (mask < 0.5)  # masked image is c h w\n",
        "\n",
        "    mask = torch.stack([torch.tensor(mask) for _ in range(num_samples)], dim=0).to(\"cuda\")\n",
        "    mask = torch.nn.functional.interpolate(mask, size=(64, 64))\n",
        "\n",
        "    if seed == -1:\n",
        "        seed = random.randint(0, 65535)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    if config.save_memory:\n",
        "        model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "    masked_image = torch.stack(\n",
        "        [torch.tensor(masked_image) for _ in range(num_samples)], dim=0\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # this should be b,c,h,w\n",
        "    masked_image = model.get_first_stage_encoding(model.encode_first_stage(masked_image))\n",
        "\n",
        "    x = torch.stack([torch.tensor(source) for _ in range(num_samples)], dim=0).to(\"cuda\")\n",
        "    z = model.get_first_stage_encoding(model.encode_first_stage(x))\n",
        "\n",
        "    cats = torch.cat([mask, masked_image], dim=1)\n",
        "\n",
        "    hint = hint[\n",
        "        None,\n",
        "    ].repeat(3, axis=0)\n",
        "\n",
        "    hint = torch.stack([torch.tensor(hint) for _ in range(num_samples)], dim=0).to(\"cuda\")\n",
        "\n",
        "    cond = {\n",
        "        \"c_concat\": [cats],\n",
        "        \"c_control\": [hint],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([prompt] * num_samples)],\n",
        "    }\n",
        "    un_cond = {\n",
        "        \"c_concat\": [cats],\n",
        "        \"c_control\": [hint],\n",
        "        \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)],\n",
        "    }\n",
        "\n",
        "\n",
        "    shape = (4, H // 8, W // 8)\n",
        "\n",
        "    if config.save_memory:\n",
        "        model.low_vram_shift(is_diffusing=True)\n",
        "\n",
        "    if not adaptive_control:\n",
        "        seed_everything(seed)\n",
        "        model.control_scales = (\n",
        "            [strength * (0.825 ** float(12 - i)) for i in range(13)]\n",
        "            if guess_mode\n",
        "            else ([strength] * 13)\n",
        "        )  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
        "        samples, intermediates = ddim_sampler.sample(\n",
        "            ddim_steps,\n",
        "            num_samples,\n",
        "            shape,\n",
        "            cond,\n",
        "            verbose=False,\n",
        "            unconditional_guidance_scale=scale,\n",
        "            unconditional_conditioning=un_cond,\n",
        "            x0=z,\n",
        "            mask=mask\n",
        "        )\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        x_samples = model.decode_first_stage(samples)\n",
        "        # print(x_samples.shape)\n",
        "        x_samples = (\n",
        "            (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 127.5 + 127.5)\n",
        "            .cpu()\n",
        "            .numpy()\n",
        "            .clip(0, 255)\n",
        "            .astype(np.uint8)\n",
        "        )\n",
        "\n",
        "        if eval: # currently only works for batch size of 1 \n",
        "            assert num_samples == 1, \"MPJPE evaluation currently only works for batch size of 1\"\n",
        "            mpjpe = meshgraphormer.eval_mpjpe(x_samples[0], info)\n",
        "            print(mpjpe)\n",
        "        for i in range(num_samples):\n",
        "            cv2.imwrite('/content/HandRefiner/output/output_image.jpg', cv2.cvtColor(x_samples[i], cv2.COLOR_RGB2BGR))\n",
        "            gen_count += 1\n",
        "            return '/content/HandRefiner/output/output_image.jpg'\n",
        "    else:\n",
        "        assert num_samples == 1, \"Adaptive thresholding currently only works for batch size of 1\"\n",
        "        strengths = [1.0, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "        ref_mpjpe = None\n",
        "        chosen_strength = None\n",
        "        final_mpjpe = None\n",
        "        chosen_sample = None\n",
        "        count = 0\n",
        "        for strength in strengths:\n",
        "            seed_everything(seed)\n",
        "            model.control_scales = (\n",
        "                [strength * (0.825 ** float(12 - i)) for i in range(13)]\n",
        "                if guess_mode\n",
        "                else ([strength] * 13)\n",
        "            )  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
        "            samples, intermediates = ddim_sampler.sample(\n",
        "                ddim_steps,\n",
        "                num_samples,\n",
        "                shape,\n",
        "                cond,\n",
        "                verbose=False,\n",
        "                unconditional_guidance_scale=scale,\n",
        "                unconditional_conditioning=un_cond,\n",
        "                x0=z,\n",
        "                mask=mask\n",
        "            )\n",
        "            if config.save_memory:\n",
        "                model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "            x_samples = model.decode_first_stage(samples)\n",
        "\n",
        "            x_samples = (\n",
        "                (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 127.5 + 127.5)\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "                .clip(0, 255)\n",
        "                .astype(np.uint8)\n",
        "            )\n",
        "            mpjpe = meshgraphormer.eval_mpjpe(x_samples[0], info)\n",
        "            if count == 0:\n",
        "                ref_mpjpe = mpjpe\n",
        "                chosen_sample = x_samples[0]\n",
        "            elif mpjpe < ref_mpjpe * 1.15:\n",
        "                chosen_strength = strength\n",
        "                final_mpjpe = mpjpe\n",
        "                chosen_sample = x_samples[0]\n",
        "                break\n",
        "            elif strength == 0.9:\n",
        "                final_mpjpe = ref_mpjpe\n",
        "                chosen_strength = 1.0\n",
        "            count += 1\n",
        "        \n",
        "        cv2.imwrite('/content/HandRefiner/output/output_image.jpg', cv2.cvtColor(x_samples[0], cv2.COLOR_RGB2BGR))\n",
        "        gen_count += 1\n",
        "        return '/content/HandRefiner/output/output_image.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_image = inference('/content/examples_IMG_1050.jpg', \"a cute rabbit, white background, pastel hues, minimal illustration, line art, pen drawing\", 1)\n",
        "output_image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
